name: Manual Benchmarks (Maintainers Only)

on:
  workflow_dispatch:
    inputs:
      protocols:
        description: 'Protocols to benchmark'
        required: true
        default: 'all'
        type: choice
        options:
          - 'all'
          - 'http'
          - 'mcp'
          - 'a2a'
      
      test_type:
        description: 'Type of benchmark test'
        required: true
        default: 'quick'
        type: choice
        options:
          - 'quick'
          - 'comprehensive'
          - 'latency'
          - 'throughput'
      
      duration:
        description: 'Test duration'
        required: true
        default: '30s'
        type: choice
        options:
          - '30s'
          - '60s'
          - '120s'
          - '300s'
      
      platform:
        description: 'Platform to run on'
        required: true
        default: 'ubuntu-latest'
        type: choice
        options:
          - 'ubuntu-latest'
          - 'ubuntu-22.04-arm'
          - 'both'
      
      notify_maintainers:
        description: 'Send notification to maintainers'
        required: true
        default: true
        type: boolean
  
  workflow_call:
    inputs:
      protocols:
        description: 'Protocols to benchmark'
        required: true
        default: 'all'
        type: string
      
      test_type:
        description: 'Type of benchmark test'
        required: true
        default: 'quick'
        type: string
      
      duration:
        description: 'Test duration'
        required: true
        default: '30s'
        type: string
      
      platform:
        description: 'Platform to run on'
        required: true
        default: 'ubuntu-latest'
        type: string
      
      notify_maintainers:
        description: 'Send notification to maintainers'
        required: true
        default: true
        type: boolean

env:
  CARGO_TERM_COLOR: always

jobs:
  check-permissions:
    runs-on: ubuntu-latest
    outputs:
      is-maintainer: ${{ steps.check.outputs.is-maintainer }}
    steps:
      - name: Check if user is maintainer
        id: check
        run: |
          # Check if the actor has admin or maintain permissions
          RESPONSE=$(curl -s -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
            "https://api.github.com/repos/${{ github.repository }}/collaborators/${{ github.actor }}/permission")
          
          PERMISSION=$(echo "$RESPONSE" | jq -r '.permission // "none"')
          
          if [[ "$PERMISSION" == "admin" || "$PERMISSION" == "maintain" ]]; then
            echo "is-maintainer=true" >> $GITHUB_OUTPUT
            echo "✅ User ${{ github.actor }} has $PERMISSION permissions"
          else
            echo "is-maintainer=false" >> $GITHUB_OUTPUT
            echo "❌ User ${{ github.actor }} has $PERMISSION permissions (admin or maintain required)"
          fi
  
  update-baselines:
    needs: check-permissions
    if: needs.check-permissions.outputs.is-maintainer == 'true'
    runs-on: ubuntu-latest
    outputs:
      baselines-updated: ${{ steps.update.outputs.updated }}
      changes-summary: ${{ steps.update.outputs.changes }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install Python dependencies
        run: |
          pip install requests beautifulsoup4 feedparser
      
      - name: Update Industry Baselines
        id: update
        run: |
          cd crates/agentgateway/benches/traffic/reports
          
          # Create baseline update script if it doesn't exist
          if [ ! -f update-baselines.py ]; then
            cat > update-baselines.py << 'EOF'
          #!/usr/bin/env python3
          """
          Dynamic baseline update system for AgentGateway benchmarks.
          Checks for updates to industry benchmark data and updates comparison baselines.
          """
          
          import json
          import hashlib
          import requests
          from datetime import datetime
          import sys
          
          # Current baselines (will be updated dynamically)
          CURRENT_BASELINES = {
              "nginx": {
                  "p95_latency_ms": 2.1,
                  "throughput_qps": 125000,
                  "source": "TechEmpower Round 23",
                  "last_updated": "2024-07-30"
              },
              "haproxy": {
                  "p95_latency_ms": 2.3,
                  "throughput_qps": 118000,
                  "source": "TechEmpower Round 23",
                  "last_updated": "2024-07-30"
              },
              "envoy": {
                  "p95_latency_ms": 3.1,
                  "throughput_qps": 95000,
                  "source": "Envoy Benchmarks 2024",
                  "last_updated": "2024-07-30"
              },
              "pingora": {
                  "p95_latency_ms": 1.8,
                  "throughput_qps": 200000,
                  "source": "Cloudflare Blog 2024",
                  "last_updated": "2024-07-30"
              }
          }
          
          def check_techempower_updates():
              """Check for TechEmpower Framework updates."""
              try:
                  # This is a placeholder - in reality we'd check TechEmpower API
                  # For now, return no changes
                  return False, {}
              except Exception as e:
                  print(f"Error checking TechEmpower: {e}")
                  return False, {}
          
          def check_vendor_updates():
              """Check vendor blogs and releases for performance updates."""
              try:
                  # This is a placeholder - in reality we'd check various vendor sources
                  # For now, return no changes
                  return False, {}
              except Exception as e:
                  print(f"Error checking vendor updates: {e}")
                  return False, {}
          
          def main():
              changes = []
              updated = False
              
              # Check various sources for updates
              te_updated, te_changes = check_techempower_updates()
              vendor_updated, vendor_changes = check_vendor_updates()
              
              if te_updated:
                  changes.extend(te_changes)
                  updated = True
              
              if vendor_updated:
                  changes.extend(vendor_changes)
                  updated = True
              
              # Output results for GitHub Actions
              print(f"updated={str(updated).lower()}")
              
              if changes:
                  changes_summary = "\\n".join([
                      f"- {change['proxy']}: {change['metric']} changed from {change['old']} to {change['new']} ({change['source']})"
                      for change in changes
                  ])
                  print(f"changes={changes_summary}")
              else:
                  print("changes=No baseline changes detected")
          
          if __name__ == "__main__":
              main()
          EOF
          fi
          
          # Run baseline update check
          python update-baselines.py >> $GITHUB_OUTPUT
  
  benchmark:
    needs: [check-permissions, update-baselines]
    if: needs.check-permissions.outputs.is-maintainer == 'true'
    strategy:
      matrix:
        os: ${{ fromJson(inputs.platform == 'both' && '["ubuntu-latest", "ubuntu-22.04-arm"]' || format('["{0}"]', inputs.platform)) }}
    
    runs-on: ${{ matrix.os }}
    timeout-minutes: 60
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Run Benchmarks
        run: |
          cd crates/agentgateway/benches/traffic/docker
          
          echo "🚀 Starting benchmarks with configuration:"
          echo "  Protocols: ${{ inputs.protocols }}"
          echo "  Test Type: ${{ inputs.test_type }}"
          echo "  Duration: ${{ inputs.duration }}"
          echo "  Platform: ${{ matrix.os }}"
          
          # Run the benchmark with specified parameters
          ./run-docker-benchmarks.sh \
            --protocols ${{ inputs.protocols }} \
            --type ${{ inputs.test_type }} \
            --duration ${{ inputs.duration }} \
            --verbose
      
      - name: Add Baseline Update Notice
        if: needs.update-baselines.outputs.baselines-updated == 'true'
        run: |
          cd crates/agentgateway/benches/traffic/results
          
          # Add baseline update notice to results
          if [ -f benchmark_summary.md ]; then
            {
              echo ""
              echo "## 📊 Baseline Updates"
              echo ""
              echo "Industry baselines were updated before this benchmark run:"
              echo ""
              echo "${{ needs.update-baselines.outputs.changes-summary }}"
              echo ""
              echo "---"
              echo ""
            } >> baseline_updates.md
            
            # Prepend to summary
            cat baseline_updates.md benchmark_summary.md > temp_summary.md
            mv temp_summary.md benchmark_summary.md
            rm baseline_updates.md
          fi
      
      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ matrix.os }}-${{ github.run_number }}
          path: crates/agentgateway/benches/traffic/results/
          retention-days: 30
      
      - name: Cleanup Old Artifacts
        run: |
          echo "🧹 Cleaning up old benchmark artifacts (keeping last 3 runs)"
          
          # This is a placeholder for artifact cleanup logic
          # In practice, we'd use GitHub API to manage artifacts
          # For now, we'll rely on the 30-day retention policy
  
  notify:
    needs: [benchmark, update-baselines, check-permissions]
    if: always() && inputs.notify_maintainers && needs.check-permissions.outputs.is-maintainer == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Determine notification status
        id: status
        run: |
          if [[ "${{ needs.benchmark.result }}" == "success" ]]; then
            echo "status=✅ Success" >> $GITHUB_OUTPUT
            echo "color=good" >> $GITHUB_OUTPUT
          elif [[ "${{ needs.benchmark.result }}" == "failure" ]]; then
            echo "status=❌ Failed" >> $GITHUB_OUTPUT
            echo "color=danger" >> $GITHUB_OUTPUT
          else
            echo "status=⚠️ Cancelled/Skipped" >> $GITHUB_OUTPUT
            echo "color=warning" >> $GITHUB_OUTPUT
          fi
      
      - name: Send Notification
        run: |
          echo "📧 Sending notification to maintainers"
          echo "Status: ${{ steps.status.outputs.status }}"
          echo "Configuration: protocols=${{ inputs.protocols }}, type=${{ inputs.test_type }}, duration=${{ inputs.duration }}"
          echo "Platform(s): ${{ inputs.platform }}"
          echo "Triggered by: ${{ github.actor }}"
          echo "Run URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          
          if [[ "${{ needs.update-baselines.outputs.baselines-updated }}" == "true" ]]; then
            echo ""
            echo "📊 Baseline Updates:"
            echo "${{ needs.update-baselines.outputs.changes-summary }}"
          fi
          
          # In a real implementation, this would send actual email notifications
          # For now, we're just logging the notification content

  unauthorized:
    needs: check-permissions
    if: needs.check-permissions.outputs.is-maintainer != 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Unauthorized Access
        run: |
          echo "❌ Unauthorized: Only maintainers can trigger benchmark workflows"
          echo "User: ${{ github.actor }}"
          echo "Required permission: admin or maintain"
          echo "Please contact a repository maintainer if you need to run benchmarks"
          exit 1
